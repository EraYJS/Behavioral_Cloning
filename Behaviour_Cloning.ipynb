{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviour Cloning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /Users/jyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset from Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetGen():\n",
    "    with open('simdata/driving_log.csv') as log_file:\n",
    "        log_reader = csv.DictReader(log_file)\n",
    "        X = []\n",
    "        y = []\n",
    "        steering_offset = 0.4\n",
    "\n",
    "        for row in log_reader:\n",
    "            centerImage = mpimg.imread(row['center'].strip().replace('/home/era/Projects/Work/simdata', 'simdata'))\n",
    "            flippedCenterImage = np.fliplr(centerImage)\n",
    "            centerSteering = float(row['steering'])\n",
    "\n",
    "            leftImage = mpimg.imread(row['left'].strip().replace('/home/era/Projects/Work/simdata', 'simdata'))\n",
    "            flippedLeftImage = np.fliplr(leftImage)\n",
    "            leftSteering = centerSteering + steering_offset\n",
    "\n",
    "            rightImage = mpimg.imread(row['right'].strip().replace('/home/era/Projects/Work/simdata', 'simdata'))\n",
    "            flippedRightImage = np.fliplr(rightImage)\n",
    "            rightSteering = centerSteering - steering_offset\n",
    "            \n",
    "            X.append(centerImage)\n",
    "            X.append(flippedCenterImage)\n",
    "            X.append(leftImage)\n",
    "            X.append(flippedLeftImage)\n",
    "            X.append(rightImage)\n",
    "            X.append(flippedRightImage)\n",
    "            \n",
    "            y.append(centerSteering)\n",
    "            y.append(-centerSteering)\n",
    "            y.append(leftSteering)\n",
    "            y.append(-leftSteering)\n",
    "            y.append(rightSteering)\n",
    "            y.append(-rightSteering)\n",
    "\n",
    "#            X = np.append(X, \n",
    "#                          np.array([centerImage, \n",
    "#                                    flippedCenterImage, \n",
    "#                                    leftImage, \n",
    "#                                    flippedLeftImage, \n",
    "#                                    rightImage, \n",
    "#                                    flippedRightImage]), \n",
    "#                         axis=0)\n",
    "                        \n",
    "#            y = np.append(y, \n",
    "#                          np.array([centerSteering, \n",
    "#                                    -centerSteering, \n",
    "#                                    leftSteering, \n",
    "#                                    -leftSteering, \n",
    "#                                    rightSteering, \n",
    "#                                    -rightSteering]), \n",
    "#                          axis=0)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = datasetGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid =  train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.BatchNormalization(input_shape=(160, 320, 3), \n",
    "                                        axis=-1,\n",
    "                                        momentum=0.99, \n",
    "                                        epsilon=0.001, \n",
    "                                        center=True, \n",
    "                                        scale=True, \n",
    "                                        beta_initializer='zeros', \n",
    "                                        gamma_initializer='ones', \n",
    "                                        moving_mean_initializer='zeros', \n",
    "                                        moving_variance_initializer='ones', \n",
    "                                        beta_regularizer=None, \n",
    "                                        gamma_regularizer=None, \n",
    "                                        beta_constraint=None, \n",
    "                                        gamma_constraint=None))\n",
    "\n",
    "    model.add(layers.Conv2D(16, \n",
    "                            kernel_size=(5, 5), \n",
    "                            strides=(1, 1), \n",
    "                            activation='relu', \n",
    "                            padding='same'))\n",
    "\n",
    "    model.add(layers.Conv2D(32, \n",
    "                            kernel_size=(5, 5), \n",
    "                            strides=(2, 2), \n",
    "                            activation='relu', \n",
    "                            padding='valid'))\n",
    "\n",
    "    model.add(layers.AveragePooling2D(pool_size=(2, 2), \n",
    "                                      strides=(1, 1), \n",
    "                                      padding='valid'))\n",
    "\n",
    "    model.add(layers.Conv2D(64, \n",
    "                            kernel_size=(3, 3), \n",
    "                            strides=(2, 2), \n",
    "                            activation='relu', \n",
    "                            padding='valid'))\n",
    "\n",
    "    model.add(layers.Conv2D(64, \n",
    "                            kernel_size=(3, 3), \n",
    "                            strides=(2, 2), \n",
    "                            activation='relu', \n",
    "                            padding='valid'))\n",
    "\n",
    "    model.add(layers.AveragePooling2D(pool_size=(2, 2), \n",
    "                                      strides=(1, 1), \n",
    "                                      padding='valid'))\n",
    "\n",
    "    model.add(layers.Conv2D(128, \n",
    "                            kernel_size=(3, 3), \n",
    "                            strides=(1, 1), \n",
    "                            activation='relu', \n",
    "                            padding='valid'))\n",
    "\n",
    "    model.add(layers.Conv2D(128, \n",
    "                            kernel_size=(3, 3), \n",
    "                            strides=(1, 1), \n",
    "                            activation='relu', \n",
    "                            padding='valid'))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(4096, activation='linear'))\n",
    "\n",
    "    model.add(layers.Dense(512, activation='linear'))\n",
    "\n",
    "    model.add(layers.Dense(64, activation='linear'))\n",
    "\n",
    "    model.add(layers.Dense(8, activation='linear'))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 160, 320, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 160, 320, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 78, 158, 32)       12832     \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 77, 157, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 38, 78, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 38, 64)        36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 17, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 35, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 33, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 33, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 54912)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              224923648 \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 227,345,597\n",
      "Trainable params: 227,345,591\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11832 samples, validate on 2088 samples\n",
      "Epoch 1/8\n",
      "  512/11832 [>.............................] - ETA: 22:20 - loss: 0.2009"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-09cd303f10b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, y_train, epochs=8, batch_size=512,\n\u001b[0;32m----> 2\u001b[0;31m           validation_data=(X_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2987\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2988\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2989\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=8, batch_size=512,\n",
    "          validation_data=(X_valid, y_valid))\n",
    "\n",
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
